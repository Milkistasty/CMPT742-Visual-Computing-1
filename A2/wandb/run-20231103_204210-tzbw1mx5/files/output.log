batch 0 --- Loss: 0.3384
batch 1 --- Loss: 0.3444
batch 2 --- Loss: 0.3129
batch 3 --- Loss: 0.3292
batch 4 --- Loss: 0.3203
batch 5 --- Loss: 0.3129
batch 6 --- Loss: 0.3351
C:\Users\Alienware\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ..\aten\src\ATen\native\TensorShape.cpp:3484.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
C:\Users\Alienware\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\nn\functional.py:4236: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
batch 7 --- Loss: 0.3520
batch 8 --- Loss: 0.3040
batch 9 --- Loss: 0.2991
batch 10 --- Loss: 0.3039
batch 11 --- Loss: 0.3377
batch 12 --- Loss: 0.3104
batch 13 --- Loss: 0.3165
batch 14 --- Loss: 0.3257
batch 15 --- Loss: 0.3247
batch 16 --- Loss: 0.3240
batch 17 --- Loss: 0.3160
batch 18 --- Loss: 0.3366
batch 19 --- Loss: 0.3121
batch 20 --- Loss: 0.3037
batch 21 --- Loss: 0.2937
batch 22 --- Loss: 0.2804
batch 23 --- Loss: 0.2886
batch 24 --- Loss: 0.3057
batch 25 --- Loss: 0.3056
batch 26 --- Loss: 0.3257
batch 27 --- Loss: 0.3220
batch 28 --- Loss: 0.3083
batch 29 --- Loss: 0.2751
batch 30 --- Loss: 0.2845
batch 31 --- Loss: 0.2986
batch 32 --- Loss: 0.3020
batch 33 --- Loss: 0.3182
batch 34 --- Loss: 0.2971
batch 35 --- Loss: 0.2759
batch 36 --- Loss: 0.3122
batch 37 --- Loss: 0.3092
batch 38 --- Loss: 0.3361
batch 39 --- Loss: 0.3035
batch 40 --- Loss: 0.2750
batch 41 --- Loss: 0.2713
batch 42 --- Loss: 0.3401
batch 43 --- Loss: 0.3429
batch 44 --- Loss: 0.2995
batch 45 --- Loss: 0.3028
batch 46 --- Loss: 0.3128
batch 47 --- Loss: 0.2768
batch 48 --- Loss: 0.2902
batch 49 --- Loss: 0.2866
batch 50 --- Loss: 0.2884
batch 51 --- Loss: 0.2670
batch 52 --- Loss: 0.2942
batch 53 --- Loss: 0.3191
batch 54 --- Loss: 0.2649
batch 55 --- Loss: 0.2651
batch 56 --- Loss: 0.2848
batch 57 --- Loss: 0.3078
batch 58 --- Loss: 0.2994
batch 59 --- Loss: 0.2713
batch 60 --- Loss: 0.3240
batch 61 --- Loss: 0.2872
batch 62 --- Loss: 0.2901
batch 63 --- Loss: 0.2952
batch 64 --- Loss: 0.2958
batch 65 --- Loss: 0.2818
batch 66 --- Loss: 0.2882
batch 67 --- Loss: 0.2585
batch 68 --- Loss: 0.2716
batch 69 --- Loss: 0.2966
batch 70 --- Loss: 0.2798
batch 71 --- Loss: 0.2990
batch 72 --- Loss: 0.3003
batch 73 --- Loss: 0.2507
batch 74 --- Loss: 0.2751
batch 75 --- Loss: 0.2721
batch 76 --- Loss: 0.2681
batch 77 --- Loss: 0.2164
batch 78 --- Loss: 0.3407
batch 79 --- Loss: 0.2724
batch 80 --- Loss: 0.2851
batch 81 --- Loss: 0.3104
batch 82 --- Loss: 0.2861
batch 83 --- Loss: 0.3117
batch 84 --- Loss: 0.2363
batch 85 --- Loss: 0.2802
batch 86 --- Loss: 0.2665
batch 87 --- Loss: 0.2226
batch 88 --- Loss: 0.3033
batch 89 --- Loss: 0.2828
batch 90 --- Loss: 0.2312
batch 91 --- Loss: 0.2988
batch 92 --- Loss: 0.2707
batch 93 --- Loss: 0.2600
batch 94 --- Loss: 0.2665
batch 95 --- Loss: 0.2444
batch 96 --- Loss: 0.3420
batch 97 --- Loss: 0.3214
batch 98 --- Loss: 0.2786
batch 99 --- Loss: 0.2560
batch 100 --- Loss: 0.3276
batch 101 --- Loss: 0.2631
batch 102 --- Loss: 0.3517
batch 103 --- Loss: 0.2481
batch 104 --- Loss: 0.2407
Epoch 1 / 5 --- Loss: 0.2953
Accuracy: 0.8156 ---- Loss: 0.2283
batch 0 --- Loss: 0.2594
batch 1 --- Loss: 0.2701
batch 2 --- Loss: 0.2700
batch 3 --- Loss: 0.2511
batch 4 --- Loss: 0.2793
batch 5 --- Loss: 0.3308
batch 6 --- Loss: 0.2699
batch 7 --- Loss: 0.2731
batch 8 --- Loss: 0.2743
batch 9 --- Loss: 0.2354
batch 10 --- Loss: 0.3227
batch 11 --- Loss: 0.2436
batch 12 --- Loss: 0.2464
batch 13 --- Loss: 0.2402
batch 14 --- Loss: 0.2473
batch 15 --- Loss: 0.2747
batch 16 --- Loss: 0.2517
batch 17 --- Loss: 0.2815
batch 18 --- Loss: 0.2551
batch 19 --- Loss: 0.2046
batch 20 --- Loss: 0.2656
batch 21 --- Loss: 0.2216
batch 22 --- Loss: 0.2618
batch 23 --- Loss: 0.2689
batch 24 --- Loss: 0.2793
batch 25 --- Loss: 0.2220
batch 26 --- Loss: 0.2858
batch 27 --- Loss: 0.2236
batch 28 --- Loss: 0.2795
batch 29 --- Loss: 0.2588
